{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 done.\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "root = '.\\\\Preprocessed2'\n",
    "paths = [os.path.join(root,i) for i in os.listdir(root)]\n",
    "dirs = [i for i in os.listdir(root)]\n",
    "len_dict = dict.fromkeys(dirs)\n",
    "idx = 0\n",
    "for folder in paths:\n",
    "    arr = []\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        \n",
    "        with open(os.path.join(folder,file), encoding='utf-8' ) as fp:\n",
    "            length = len(fp.read().split())\n",
    "        arr.append(length)\n",
    "\n",
    "    len_dict[dirs[idx]] = arr\n",
    "    print(str(idx)+' done.',end='\\r')\n",
    "    idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business': [7.0, 80.0, 139.0, 253.5, 2090.0],\n",
       " 'Education': [0.0, 102.0, 150.0, 227.0, 2670.0],\n",
       " 'Entertainment': [0.0, 90.0, 133.0, 248.0, 10812.0],\n",
       " 'Health': [13.0, 86.0, 140.0, 243.0, 2592.0],\n",
       " 'Interview': [44.0, 579.0, 880.0, 1342.25, 7769.0],\n",
       " 'Literature': [24.0, 127.0, 228.0, 741.5, 5772.0],\n",
       " 'NationalNews': [22.0, 86.0, 135.0, 236.0, 1069.0],\n",
       " 'Opinion': [221.0, 700.0, 898.0, 1148.0, 6984.0],\n",
       " 'Sports': [20.0, 80.0, 111.0, 167.5, 2429.0],\n",
       " 'Technology': [39.0, 140.0, 190.5, 283.0, 2705.0],\n",
       " 'Tourism': [22.0, 126.0, 198.0, 326.75, 3813.0],\n",
       " 'World': [25.0, 93.0, 126.0, 171.0, 1445.0]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output_dict is a dictionary that as key:value pair where Key is Folder name (Business, Education,... ) and value is a list containing five number summary.\n",
    "# e.g. \n",
    "# 'folder_name':[Minimum, Quartile 1, Median, Quartile 3, Maximum]\n",
    "# 'Business': [7.0, 80.0, 138.5, 252.0, 2090.0]\n",
    "\n",
    "from numpy import percentile\n",
    "output_dict = dict.fromkeys(dirs)\n",
    "idx = 0\n",
    "for i in len_dict.values():\n",
    "    arr = []\n",
    "    five_number_summary = percentile(i,[0,25,50,75,100])\n",
    "    arr.append(five_number_summary[0])\n",
    "    arr.append(five_number_summary[1])\n",
    "    arr.append(five_number_summary[2])\n",
    "    arr.append(five_number_summary[3])\n",
    "    arr.append(five_number_summary[4])\n",
    "    output_dict[dirs[idx]]=arr\n",
    "    idx+=1\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1452\n",
      "1325\n",
      "1337\n",
      "1328\n",
      "1387\n",
      "1244\n",
      "1465\n",
      "1327\n",
      "1333\n",
      "1379\n",
      "1398\n",
      "1397\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for i in dirs:  \n",
    "    a.append([num for num in len_dict[i] if num>=output_dict[i][1] and num<=output_dict[i][3]])\n",
    "for i in a:\n",
    "    print(len(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446\r"
     ]
    }
   ],
   "source": [
    "root = '.\\\\Preprocessed\\\\Business'\n",
    "index = 1\n",
    "for i in os.listdir(root):\n",
    "    with open(os.path.join(root,i),encoding='utf-8') as fp:\n",
    "        tsize = len(fp.read().split())\n",
    "        \n",
    "        if tsize<=80.0 or tsize>=252.0:\n",
    "            print( index,end='\\r')\n",
    "            index+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '.\\\\FilteredPreprocessed2'\n",
    "dirs = [i for i in os.listdir(root)]\n",
    "dirs_path = [os.path.join(root,i) for i in dirs]\n",
    "idx1=0\n",
    "for folder in dirs_path:\n",
    "    for files in os.listdir(folder):\n",
    "        with open(os.path.join(folder,files),encoding='utf-8') as fp:\n",
    "            tsize = len(fp.read().split())\n",
    "        field = dirs[idx1]\n",
    "        \n",
    "        if tsize<= output_dict[field][1] or tsize>= output_dict[field][3]:\n",
    "            os.remove(os.path.join(folder,files))\n",
    "    idx1+=1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the words which occur once in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_extraction import tfidfvectorizer\n",
    "import os\n",
    "def cUniqueWords(): # counts unique words in the filteredPreprocessed2 folder \n",
    "    counter = tfidfvectorizer.TfidfVectorizer()\n",
    "    root = \".\\\\FilteredPreprocessed2\\\\\"\n",
    "    for folder in os.listdir(root):\n",
    "        dir = os.path.join(root, folder)\n",
    "        for files in os.listdir(dir):\n",
    "            fpath = os.path.join(dir, files)\n",
    "            counter.vocabulary(path=fpath)\n",
    "    return counter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = cUniqueWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109127"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_noisy_words(words_dict):\n",
    "    noisy = []\n",
    "    for k, v in words_dict.items():\n",
    "        if v[0] == 1:\n",
    "            noisy.append(k)\n",
    "    return noisy\n",
    "\n",
    "\n",
    "noisy_words = count_noisy_words(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56646"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noisy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['नहाए', 'केरालगाय', 'हाजीपुर', 'सीतामढी', 'अङ्गिकृत', 'यमपञ्च', 'दण्डपाणी', 'बानियाँटार', 'क्याङ्कर', 'हाल्नै', 'नफल्ने', 'कलकत्ते', 'फूल्न', 'थङ्गा', 'हरके', 'खेल्छु', 'होमिन', 'नपुग्नु', 'फाइनान्सियर', 'भएसँंगै', 'स्ट्रयान्डेड', 'रिपाट्रिएशन', 'सजनगोठ', 'गंगौर', 'स्विट्स', 'खजुराखुर्द', 'भुमिसरा', 'ट्रान्ज्याक्सन', 'फिङ्गरप्रिन्ट', 'ट्रान्सज्याक्सन', 'सिर्फट', 'छपाईं', 'क्यालिब्रेशन', 'एन्फास्ट्रक्चर', 'रिट्ठेपानी', 'ल्याण्डफिल्ड', 'राखेको', 'बेको', 'तीब्रतर', 'पनिराष्ट्र', 'चम्काए', 'नियमत', 'घटन', 'क्रियाकलपा', 'सर्भेयरलगायत', 'चुक्तापुँजी', 'एटीएम्', 'बैंकिङ्क', 'एकाउण्ट', 'आइआइएक्स', 'अहेड', 'मणीरत्न', 'मणिरत्न', 'तक्का', 'विश्वासयोग्य', 'रुपैयाँसम्महाल', 'असर्फीपौडेल', 'बिक्रीकोटा', 'किन्नसक्ने', 'घुईँचो', 'सीएमओ', 'बिसुनदास', 'हेप्', 'ताग्रु', 'सल्ली', 'यङगार', 'लेप्चे', 'स्याम्ने', 'सुनाखडा', 'गोठी', 'न्यासी', 'प्रत्साहन', 'सहुलियता', 'चनरवती', 'सान्ती', 'दावीकर्ता', 'लोहन्द्रा', 'रुआठ', 'चिस्याङ', 'दसबरहा', 'पाँचनदी', 'रुचार', 'चिस्याङखोला', 'लोहन्दा', 'दंसबक्राहा', 'जनाएकोछ', 'चारस्थान', 'भीलानन्द', 'बहकिने', 'आईपिओ', 'उद्देश्यस', 'सहकार्यक्र', 'सागरदेव', 'रुपेशकृष्ण', 'तेजिङ', 'गोन्सर', 'लतिका', 'कार्टेलिङ', 'डब्लूटीओ', 'रोक्नैपर्छ']\n"
     ]
    }
   ],
   "source": [
    "print(noisy_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the vocabulary of the filtered processed files to \"vocab2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab2.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"word => count, index\\n\")\n",
    "    for k, v in unique.items():\n",
    "        fp.write(k + \" =>\" + str(v[0])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords2.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    for word in noisy_words:\n",
    "        fp.write(word + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now deleting all those words which occur only once in the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Literature', 'NationalNews', 'Sports', 'Technology', 'Tourism', 'World']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root = \".\\\\FilteredPreprocessed2\\\\\"\n",
    "dirs = os.listdir(root)\n",
    "len(dirs)\n",
    "dirs = dirs[4:]\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delWords(dirs):\n",
    "    \n",
    "    with open('.\\\\stopwords2.txt',encoding=\"utf-8\") as sw:\n",
    "        stopwords = (sw.read()).split()\n",
    "    # print(len(stopwords))\n",
    "    for folder in dirs:\n",
    "        dir = os.path.join(root, folder)\n",
    "        files =os.listdir(dir)\n",
    "        print(folder + \".........\", end=\"\\r\")\n",
    "        no = 0\n",
    "        total = len(files)\n",
    "        for file in files:\n",
    "            no += 1\n",
    "            left = total - no\n",
    "            processed_text = []\n",
    "            fpath = os.path.join(dir, file)\n",
    "            with open(fpath, \"r\", encoding=\"utf-8\") as fp:\n",
    "                text = fp.read().split()\n",
    "                \n",
    "                for word in text:\n",
    "                    if word not in stopwords:\n",
    "                        processed_text.append(word)\n",
    "                \n",
    "            processed_text = ' '.join(processed_text)\n",
    "            with open(fpath,\"w\", encoding=\"utf-8\") as fp:\n",
    "                fp.write(processed_text)\n",
    "            print(f\"{no} of files done. {left} out of {total}.........\", end=\"\\r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1363 of files done. 0 out of 1363...........\r"
     ]
    }
   ],
   "source": [
    "delWords(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = cUniqueWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52481"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a document-term matrix of the words....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.feature_extraction import tfidfvectorizer\n",
    "\n",
    "def createDT(path=\".\\\\FilteredPreprocessed2\"):\n",
    "    dirs = os.listdir(path)\n",
    "    vectorizer = tfidfvectorizer.TfidfVectorizer()\n",
    "    vectorizer.documentTermMat(Path=path)\n",
    "    return vectorizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def createDocSets():\n",
    "    root = './FilteredPreprocessed'\n",
    "    dest = './DocSets1'\n",
    "    dirs = [i for i in os.listdir(root)]\n",
    "\n",
    "    all_sets = {}\n",
    "    for folder in dirs:\n",
    "        total = 0\n",
    "        files = os.listdir(os.path.join(root, folder))\n",
    "        file_index = [i for i in range(len(files))] #get file index\n",
    "        set_list = []\n",
    "        for i in range(4): #four unique sets\n",
    "            smaller_set = random.sample(file_index, len(files) // 4) #get a sample of size one-fourth of the total files\n",
    "            file_index = [i for i in file_index if i not in smaller_set] #deletes all the file indexes which are already sampled\n",
    "            print(len(file_index))\n",
    "            os.makedirs(os.path.join(dest , \"Set\" + str(i), folder), exist_ok=True)\n",
    "            for sm in smaller_set:\n",
    "                shutil.copy(os.path.join(root, folder, files[sm]), os.path.join(dest, \"Set\" + str(i), folder))    \n",
    "            set_list.append(smaller_set)\n",
    "        all_sets[folder] = set_list #dict of all the unique sets of documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating 4 sets of datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createDocSets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter = createDT(path=\".\\\\DocSets1\\\\Set0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter.vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.feature_extraction import plsa\n",
    "\n",
    "    \n",
    "\n",
    "p = plsa.PLSA(counter.vec[:, :])\n",
    "p.plsa(10, 25, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p.document_topic_prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p.topic_word_prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open(\".\\\\pickles\\\\Set1\\\\vocab.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(counter1.vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set1\\\\docTerm.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(counter1.vec, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set1\\\\doc_t_prob.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(p.document_topic_prob, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set1\\\\t_word_prob.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(p.topic_word_prob, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter1 = createDT(path=\".\\\\DocSets1\\\\Set1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter1.vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.feature_extraction import plsa\n",
    "\n",
    "    \n",
    "\n",
    "p = plsa.PLSA(counter1.vec)\n",
    "p.plsa(10, 30, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open(\".\\\\pickles\\\\Set1\\\\vocab.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(counter1.vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set1\\\\docTerm.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(counter1.vec, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set1\\\\doc_t_prob.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(p.document_topic_prob, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set1\\\\t_word_prob.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(p.topic_word_prob, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter2 = createDT(path=\".\\\\DocSets1\\\\Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter2.vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = plsa.PLSA(counter2.vec)\n",
    "p.plsa(10, 30, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set2\\\\vocab.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(counter2.vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set2\\\\docTerm.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(counter2.vec, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set2\\\\doc_t_prob.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(p.document_topic_prob, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set2\\\\t_word_prob.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(p.topic_word_prob, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter3 = createDT(path=\".\\\\DocSets1\\\\Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = plsa.PLSA(counter3.vec)\n",
    "p.plsa(10, 30, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set3\\\\vocab.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(counter3.vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set3\\\\docTerm.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(counter3.vec, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set3\\\\doc_t_prob.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(p.document_topic_prob, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\".\\\\pickles\\\\Set3\\\\t_word_prob.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(p.topic_word_prob, fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33dc6d49505b4536b6a128d9d7c879e1fa44477ad44947bbbe73093067fe6393"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
