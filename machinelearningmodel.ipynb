{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessing import preprocessing\n",
    "import os \n",
    "\n",
    "# don't forget to change the '/' in the filename to '\\\\' in windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dirs = os.listdir(\"./16NepaliNews/raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no need to run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories(folder): # no need to run this now\n",
    "    for dir in dirs:\n",
    "        os.makedirs(os.path.join(folder,dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(dir, filename, text):\n",
    "    folder_name = \"Preprocessed\"\n",
    "    path = \".\\\\\" + os.path.join(folder_name, dir, filename )\n",
    "    \n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nepali_stemmer.stemmer import NepStemmer\n",
    "\n",
    "\n",
    "def preProcess(folder_start, folder_end):\n",
    "    punc = preprocessing.Preprocessing()\n",
    "    #running the simple preprocessing for all files\n",
    "    for folder in dirs[folder_start: folder_end]: \n",
    "        dir = os.getcwd() + \"/16NepaliNews/raw/\" + folder\n",
    "        print(folder)\n",
    "        for file in os.listdir(dir): #remove the list value [:1]\n",
    "            filename = dir + \"/\" + file\n",
    "            \n",
    "            \n",
    "            \n",
    "            text = punc.delPnctuatn(filename=filename, next=True)\n",
    "            \n",
    "            \n",
    "            text = punc.delEngWords(text=text, next=True)\n",
    "            \n",
    "            \n",
    "            text = punc.delNumbers(text=text, next=True)\n",
    "            \n",
    "            text = punc.delStopwords(text=text, next=True)\n",
    "            \n",
    "            stem = NepStemmer()\n",
    "            text = stem.stem(text)\n",
    "            write_file(folder, file, text)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_extraction import tfidfvectorizer\n",
    "\n",
    "counter = tfidfvectorizer.TfidfVectorizer()\n",
    "# counter.vocabulary(path=\".\\\\Preprocessed\\\\Technology\\\\04_11_21_10_39_40.txt\")\n",
    "# counter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n",
      "1408\n",
      "Education\n",
      "1296\n",
      "Entertainment\n",
      "1309\n",
      "Health\n",
      "1301\n",
      "Interview\n",
      "1383\n",
      "Literature\n",
      "1237\n",
      "NationalNews\n",
      "1440\n",
      "Opinion\n",
      "1321\n",
      "Sports\n",
      "1295\n",
      "Technology\n",
      "1372\n",
      "Tourism\n",
      "1380\n",
      "World\n",
      "1366\n"
     ]
    }
   ],
   "source": [
    "def count_words():\n",
    "    for folder in dirs:\n",
    "        dir = os.getcwd() + \"\\\\FilteredPreprocessed\\\\\" + folder\n",
    "        print(folder)\n",
    "        count = 0\n",
    "        \n",
    "        for file in os.listdir(dir):\n",
    "            filename = dir +\"\\\\\" + file\n",
    "            counter.vocabulary(path=filename)\n",
    "            count+=1\n",
    "        print(count)\n",
    "count_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169205"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(1000):\n",
    "#     print(counter.vocab[list(counter.vocab.keys())[i]], list(counter.vocab.keys())[i])\n",
    "    \n",
    "len(counter.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\\\vocab.txt\",\"w\", encoding=\"utf-8\") as fp:\n",
    "    for key in counter.vocab.keys():\n",
    "        fp.write(key + \" \" + str(counter.vocab[key])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 left of 1366 of World                                          \r"
     ]
    }
   ],
   "source": [
    "from src.feature_extraction import tfidfvectorizer\n",
    "vectorizer = tfidfvectorizer.TfidfVectorizer()\n",
    "vectorizer.documentTermMat('.\\\\FilteredPreprocessed\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1811, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocab['कक्षा']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1811.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vec[:,:1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.2 GiB for an array with shape (16108, 169205) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32me:\\Project\\NewsTextClassifierSeventhProject\\machinelearningmodel.ipynb Cell 14'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Project/NewsTextClassifierSeventhProject/machinelearningmodel.ipynb#ch0000013?line=0'>1</a>\u001b[0m vectorizer\u001b[39m.\u001b[39;49mtf_idf()\n",
      "File \u001b[1;32me:\\Project\\NewsTextClassifierSeventhProject\\src\\feature_extraction\\tfidfvectorizer.py:184\u001b[0m, in \u001b[0;36mTfidfVectorizer.tf_idf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=171'>172</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtf_idf\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=172'>173</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=173'>174</a>\u001b[0m \u001b[39m        term-frequency (tf) =   (Number of repeatition of words in document)\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=174'>175</a>\u001b[0m \u001b[39m                                --------------------------------------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=181'>182</a>\u001b[0m \u001b[39m        tf-idf = tf * idf\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=182'>183</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=183'>184</a>\u001b[0m     tf\u001b[39m=\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvec\u001b[39m/\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvec\u001b[39m.\u001b[39;49msum(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[:,np\u001b[39m.\u001b[39;49mnewaxis])\n\u001b[0;32m    <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=185'>186</a>\u001b[0m     idf\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvec\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39mcount_nonzero(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvec,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m    <a href='file:///e%3A/Project/NewsTextClassifierSeventhProject/src/feature_extraction/tfidfvectorizer.py?line=187'>188</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult\u001b[39m=\u001b[39mtf\u001b[39m*\u001b[39midf\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.2 GiB for an array with shape (16108, 169205) and data type float32"
     ]
    }
   ],
   "source": [
    "vectorizer.tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e19f2ee28a2596421b32a582284268244e387339577b4a62b40e9936a7c1aa89"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
