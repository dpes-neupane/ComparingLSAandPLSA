
एप्पल फोटोजको एक फिचरले अचानक सोसल मिडिया प्लाटफर्ममा हंगामा मच्चाएको छ । खासगरी महिलाहरु यसलाई लिएर चिन्तित देखिएका छन् ।
एप्पलको मेसिन लर्निङ तथा आर्टिफिसियल इन्टेलिजेन्सले आइफोनको फोटो लाइब्रेरी एप ‘फोटोज’ मा तस्वीरहरुको एउटा यस्तो फोल्डर बनाउँदछ जसका कारण महिलाहरुले लज्जित हुनुपरिरहेको छ ।
हालै एक जना महिलाले ट्विटरमा एप्पल फोटोज एपको यस्तो फिचरको खुलासा गरिन् । फोटो एपमा brassiere सर्च गर्दा प्रयोगकर्ताले कतिपय अति नै निजी तथा गोप्य तस्वीरहरु देखा परे । यी खास तस्वीरहरु महिला प्रयोगकर्ताले आफ्नो फोनबाट ब्रा, अन्य भित्री वस्त्र तथा बिकिनीमा तस्वीर क्लिक गरेका हुन् ।
विश्वभरका महिलाहरुले एप्पल फोटोजको यो फिचरको आलोचना गरिरहेका छन् । यो एप्पलको आर्टिफिसियल इन्टेलिजेन्सको एक सामान्य गल्तीको परिणाम भएको बताइएको छ ।
एप्पल  फोटोजमा प्रयोग गरिएको मेसिन लर्निङले एकैखाले तस्वीरहरुलाई कुनै एक ट्यागको माध्यमबाट एकत्रित गरी वर्गिकरण गर्दछ । यसको मनसाय नै प्रयोगकर्ताले आइफोनको लाइब्रेरीमा सजिलैसँग कुनै खास तस्वीर खोजेर फेला पार्न सकून् भन्ने हो । सोही सिद्धान्त अनुसार फोटोज एपमा दचबककष्भचभ ट्यागमा अर्धनग्न तस्वीर एकत्रित हुन्छन् ।
यसबाट एउटा गम्भीर चिन्ता के पैदा भएको छ भने कतै एप्पलले यस्ता तस्वीर लगायतका प्रयोगकर्ताका डाटाहरु एकत्रित गरिरहेको त छैन ? यदि हो भने आइफोन प्रयोगकर्ताको गोपनियता खतरामा पर्न सक्छ । तर एप्पलले धेरै चोटी के प्रष्टिकरण दिइसकेको छ भने उसको सफ्टवेयर फेसियल रिकग्निसन, अब्जेक्ट तथा सन डिटेक्सन केवल एप्पल डिजाइसमा मात्र सिमित हुन्छ र एप्पल कम्पनीले प्रयोगकर्ताका कुनैपनि डाटा आफुसँग राख्दैन । अर्थात आइफोन प्रयोगकर्ताका यस्ता अर्धनग्न तस्वीर लगायतका डाटाहरु सुरक्षित हुन्छन् ।
एप्पलले आफ्नो आइओएस टेन यता इमेज रिकग्निसकन आर्टिफिसियल इन्टेलिजेन्सको प्रयोग गर्दै आइरहेको छ । यस्तै खाले वर्गिकरण गुगल फोटोजमा पनि हुने गरेको छ ।
एजेन्सी
